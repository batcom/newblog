# 压力测试

<!--标题：压力测试｜分类：shell｜标签：性能,压力,测试,ab,wrk,nginx -->

> 尽可能系统的记录学习一次性能调优经验，以前都弄却没有得出什么结果。也没什么记录。这次弄，发现性能调优的确是个蛮难的问题。有时候知识面不够的时候，甚至不知无从下手。这里尽量记录一个简单可行的方案。

---

## 遇到的问题

### QPS极低

走了不少弯路，主要是思路不够清晰，一开始上来`ab`就直接压测`php-fpm`发现QPS极低，就一同乱搞，也没什么思路，搞日志，进程什么的，也不知道要为什么那样搞。时间浪费不少。现在搞到这里发现，还是要有一个固定的思路。就是肯定先测试nginx的性能，然后在看后端，应该有这么个测试链路和先后。然后就直接压测`nginx`静态文件，发现依旧很低，只有不到几十，太不可思议了。这时候又是没什么思路，一同乱搞。后来搜索到压测可能存在的坑

![](https://gitee.com/batcom/static/raw/master/images/1.png)

这里发现有可能是压测工具的问题，果然，换了`wrk`后发现并发有提高，至此静态文件压测终于有些眉头看起来正常些，可以正常调优优化了，大概单机有`6000`多吧，后来发现原来`ab`压测的时候需要加`-k`参数，否则总是只有几十`QPS`。但是`ab` 总是会出现一些问题，而且性能也不行，得不到`wrk`那么高的数据，可能和不够现代化有关，所以，以后压测直接选择`wrk`，主要还有一点就是`wrk`支持`lua`脚本动态传参，这是其它压测工具都不具备的，而且在这众多压测工具中`wrk`的性能也是最好最简单的。`Windows`下面就直接用`wrk`的`docker`就可以了，Linux下面可以考虑编译

```bash
docker run -it --rm williamyeh/wrk -t100 -c1000 -d1m "http://qps.test.me/robots.txt"
```

类似这样就可以了.

### Wsl too open many files 修改不生效的原因

又浪费不少时间，其实不是不生效，而是`wsl`不能够像真正`linux`那样重启，即便重启`docker`也是，重启了很多遍，以为没效果，后来思考了下，wsl装了个sshd，然后登录进去就发现生效了。实际上就是现在还不知道怎么在windows Terminal里面刷新wsl系统会话的方法，实在不行就只能sshd 

### Nginx 出现`10240 worker_connections exceed open file resource limit: 1024` 

这个首先一般`limit.conf`都优化过，其实最后有用的是在`nginx.conf`主上下文中他添加`worker_rlimit_nofile 65535`;即可

### PHP代码qps低

到这里，一般的思路就是先测试静态文件压力了

![](https://gitee.com/batcom/static/raw/master/images/20210610095104.png)

可以多次取平均值。

然后再压测最简单的`php hello world`示例发现QPS和静态文件差不多没什么损耗，加大进程数量也是如此，而且CPU和内存利用率也不高，而用`Tp`框架输出的QPS却只有300多，说明瓶颈在框架或者代码中的数据库，猜测极有可能是数据库，因为如果是代码性能的话，那么增加进程数量应该会提高QPS，然而一直都是380多

直接压测`test.php` 写出`Hello World`的例子

![](https://gitee.com/batcom/static/raw/master/images/20210610095847.png)

发现qps并没有损耗。

![](https://gitee.com/batcom/static/raw/master/images/20210610100616.png)

qps损耗巨大.

### 安装php性能分析工具tideways

没有条件安装`听云apm`的，可以试试开源的`tideways_xhprof+php-monitor`，实际体验效果还不错，可以一定程度上进行性能分析，后续会对比`听云APM`,`SwooleTracker`进行对比分析，后面两个不开源，也不可自主，要享受高级服务需要交钱，所以，不考虑这的前提下也会对比下。

1. 安装很简单，按照github上面操作就可，一般问题不大。`tideways_xhprof`扩展可以[直接release页面下载](https://github.com/tideways/php-xhprof-extension/releases)，linux直接安装,安装后扩展被解压到`/usr/lib/tideways_xhprof`文件夹下(**文档里有说明**)，选择对应的版本，添加到`php.ini`里去即可。Windows也是采用这种方式`dll`文件，直接下载二进制文件最方便，如果此方法不行得情况下可以采取编译方法，也很简单。
2. `php-monitor` 安装也很方便，按自己的知识面和经验遇到问题也都能自然很快速的解决，没什么好记录的。就不细说了。

安装好后，使用结果如下

![](https://gitee.com/batcom/static/raw/master/images/20210610140806.png)

点击执行时间排序，然后点击请求时间进入详情进行分析

![](https://gitee.com/batcom/static/raw/master/images/20210610140934.png)

最重要的是`sql`而我现在是从最简单的测试，还没执行sql，所以，还有下面最重要的函数分析执行时间

![](https://gitee.com/batcom/static/raw/master/images/20210610141220.png)

一下子就找到了性能所在之处了，是因为`route/config.php`里面加载了`1000`多条路由，解析这些路由花费了大量时间。所以，尝试把`config.php`里面的内容该为压测的一条，果然验证了才想，也验证了这个工具还是可以的。下面是`1000`多条路由压测和一条路由压测的结果

1000条QPS只有可怜的27，而且CPU也被耗尽，比1条的用的厉害，2核几乎都是100%(**静态开启70个php-fpm进程**)

![](https://gitee.com/batcom/static/raw/master/images/20210610141541.png)

修改1调后压测

![](https://gitee.com/batcom/static/raw/master/images/20210610141632.png)

提升了将近`3.5`倍，这算350%的提高，相当不错了。所以，觉得这个工具和方向是对了的。可以这样搞。然后，后面还将调研为什么和静态以及单独输出`hello world`进`3000`的QPS相差这么多。（**是因为安装php-monitor损失了近10倍的性能**）

### 为什么含有简单逻辑的框架接口`app_conf`和`hello`Qps 有如此大区别

可以看到排除`route`问题，进行优化后，页面响应时间已经到达最高`486.4ms`了，已经正常了。

![](https://gitee.com/batcom/static/raw/master/images/20210610142441.png)

说明不是`php`代码的问题了，但QPS依旧很高，继续优化，寻找瓶颈。搞了好久，终于能解释所有问题了，也能明白寻找瓶颈是什么意思了。这次涨太多的经验了。尽量把思路整理清楚，记录下来。

1. 实验过程中为了更好的分析数据，把上面的`php-monitor`的存储方案换成了`mysql`不是本地后，突然发现压测最基本的`php hello`Qps也只有不到500，下降严重，当时不晓得是为什么，后来乱捣鼓才知道原来瓶颈是网络，因为服务器带宽只有`1m`，而进行数据分析的存储`rds`在远程，写数据库的过程中也会占用大量的网络,所以，就会导致即便是最基本的`php hello`错误日志中也会出现`recv() failed (104: Connection reset by peer) while reading response header from upstream`的经典错误了，网络带宽不够，所以超时了.上图。

   ![](https://gitee.com/batcom/static/raw/master/images/20210610165108.gif)

   从图里我们也看到分析性能的方法了，要用`bmon/nload`监控网络(分别在图中的右上角和左上角),然后又下角的`htop`监控`CPU`和`Mem`图中，可以发现带宽一直都在1m以下而且很稳定，Cpu和内存的情况也正常，左下角图也没有出现错误日志，所以最后得到了`2204`的QPS.

   我们再来重现增加`php-monitor+mysql(远程)`方案后，QPS急速下降，并且出现大量`104 recv`错误，是由于网络瓶颈的原因的操作截图

   ![](https://gitee.com/batcom/static/raw/master/images/20210610171714.gif)

   我们看到果然网络达到1.8M峰值的时候，开始出现`104 recv`错误，Cpu也比之前多一些损耗，全部用在发送mysql插入数据了，最后我们就得到了`527`的QPS.通过这两个动态图片对比，我们更加清楚了该怎么分析瓶颈了，就是把网络监控，`htop`监控,日志监控进行分析，我们就能得到很好的结果了，如果是`php`性能可以通过上面工具进行分析，这里我们也发现`php-monitor`的一个坑，尽量变为本地压测。而其它瓶颈我们就能通过我们的监控图得出到底是网络瓶颈，CPU，还是内存了。另外进程网络监控可以用`nethogs`

所以现在就来测试下不加`php-monintor`的`app_conf`的压测结果，监控图如下

![image-20210610173552422](C:\Users\coolnet\AppData\Roaming\Typora\typora-user-images\image-20210610173552422.png)

```bash
docker run -it --rm williamyeh/wrk -t100 -c1000 -d1m --latency "http://qps.mgc-games.com/api/v7/app_conf"
Running 1m test @ http://qps.mgc-games.com/api/v7/app_conf
  100 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   544.05ms  504.51ms   1.95s    70.62%
    Req/Sec    17.13     10.54    90.00     69.53%
  Latency Distribution
     50%  256.25ms
     75%    1.19s
     90%    1.32s
     99%    1.64s
  65797 requests in 1.00m, 26.35MB read
  Socket errors: connect 0, read 4, write 607, timeout 1430
Requests/sec:   1094.83
Transfer/sec:    449.03KB
```

压测过程中没有出现错误，网络也正常，但是Cpu占用率满了，最后也不在是之前的不到`100`的QPS了，达到了现在的`1094`，有了几乎10倍(1000%)的提高，原来优化过是94，这个提高是惊人的，最重要的是我们也知道为什么不能够再提升了，瓶颈在哪里了，要解决的话，就只能是增加CPU了。

下面放张服务器基础配置图

![](https://gitee.com/batcom/static/raw/master/images/20210610174440.png)

最后测试下，加上`php-monitor`本地监控，最终的性能，损耗多少

```bash
docker run -it --rm williamyeh/wrk -t100 -c1000 -d1m --latency "http://qps.mgc-games.com/api/v7/app_conf"
Running 1m test @ http://qps.mgc-games.com/api/v7/app_conf
  100 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.61s   220.79ms   2.00s    92.08%
    Req/Sec     2.86      4.14    50.00     86.45%
  Latency Distribution
     50%    1.65s
     75%    1.70s
     90%    1.75s
     99%    1.89s
  5240 requests in 1.00m, 2.06MB read
  Socket errors: connect 0, read 0, write 712, timeout 2499
  Non-2xx or 3xx responses: 510
Requests/sec:     87.19
Transfer/sec:     35.01KB
```

发现QPS又掉到几十了，也出现`recv 104`错误，说明`php-monitor`本身和数据库的连接影响了性能，可以测试下

```bash
docker run -it --rm williamyeh/wrk -t100 -c1000 -d1m --latency "http://qps.mgc-games.com/api/v7/app_conf"
Running 1m test @ http://qps.mgc-games.com/api/v7/app_conf
  100 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.31s   469.90ms   2.00s    65.46%
    Req/Sec     3.66      4.48    70.00     82.05%
  Latency Distribution
     50%  994.67ms
     75%    1.94s
     90%    1.97s
     99%    1.99s
  9459 requests in 1.00m, 3.69MB read
  Socket errors: connect 0, read 1, write 182, timeout 4496
  Non-2xx or 3xx responses: 1156
Requests/sec:    157.40
Transfer/sec:     62.87KB
```

结果依旧不理想，因为发现这个过程中`Cpu 100%`系统负载很高。 

### 整个结果测试梳理

发现写到这里时，再从头看依旧觉得很混乱，没有对比性，也有些地方不知道为什么，所以，特地从头整理了一下结果，但是方法已经掌握，而且可以做出一些正常的猜测，这是这次的收获，有了这，整理出合理的数据就很简单了。

|                             情景                             |  QPS  | 瓶颈 |
| :----------------------------------------------------------: | :---: | :--: |
|                         1. 静态资源                          | 3600  | CPU  |
|                  2. 单纯php输出hello world                   | 3600  | CPU  |
|                    3. 多条route+app_conf                     |  357  | CPU  |
|                     4. 1条route+app_conf                     | 1079  | CPU  |
|              5. 带phpminitor多条route+app_conf               | 27.68 | CPU  |
|     6. 带phpminitor+1条route+app_conf(不修改monitor代码)     | 94.84 | 网络 |
|          7. phpminitor+1条route+app_conf+直接return          |  186  | CPU  |
|  8. phpminitor+1条route+app_conf+按执行时间过滤sql减少流量   |  198  | CPU  |
| 9. phpminitor+1条route+app_conf+注释profile和server字段方式减少流量 |  146  | CPU  |

对以上表格的一些解释，能够说明，现在对性能分析有一定经验和收获了。也能够理清上面的所有问题了。

1. 多条和单条明显有性能上几乎3倍的差距，这从3/4,5/6这两组对比都可以看出来，有3倍性能的损耗。
2. 因1的结论是正确的，所以，可以得到`phpmonitor`做性能分析是可以的。
3. 前四组没装`phpmonitor`性能普遍高于后面对照组，所以`phpmonitor`带来了巨大的性能损耗，几乎是10倍。所以，对其使用可以进行单个访问分析，结果是靠谱的，不适合并发压测分析
4. 这里实验中发现一个有趣的对照6/7组，第6组产生的瓶颈是网络,cpu是不会跑满的，而第7组则是Cpu，这体现了性能分析中网络，`htop`,日志必须同时打开进行分析的道理。
5. 而后面的7/8/9组正是为对照上面的4增加的测试，果然减少流量之后性能瓶颈又回到CPU上了，而且QPS没有发生多大变化。因为不同的因素很少了。
6. 9组之所以更小一点，是因为没有减少数据库的插入，证明数据库操作会有性能损失，但目前看来不大，瓶颈还不在此。
7. 2/4说明框架性能损失几乎有3倍，这也是很惊讶的一点，以前听别人说php性能差，没想到会有这么大。所以，下面就是分析这个部分的原因和解决方案了

这就是以上的整理和说明了，梳理的很清楚，也很容易解释。

关于上面第7点引申出来的问题，个人觉得是`php-fpm`模型的问题，改用异步模型应该会有很大改善。但还没有实践，等到时候补上实践结果。

